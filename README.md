# ADAM: Desarrollo Avanzado de Arquitecturas y Modelos para NLP

## Resumen
Este proyecto presenta ADAM (Desarrollo Avanzado de Arquitecturas y Modelos para NLP), una exploración integral de técnicas y arquitecturas innovadoras en el Procesamiento de Lenguaje Natural (NLP). El enfoque principal de este proyecto es investigar metodologías menos comunes pero con gran potencial. Las áreas clave de exploración incluyen modelos de memoria dinámica, transformadores reversibles, mecanismos de atención jerárquica y estructuras de modelos híbridos. A través de experimentos y análisis, ADAM tiene como objetivo probar el rendimiento, la eficiencia y la adaptabilidad de los modelos de NLP localmente.

## Introducción
El Procesamiento de Lenguaje Natural (NLP) ha visto avances tremendos con la aparición de grandes modelos de lenguaje (LLM) como BERT y GPT. A pesar de su éxito, existe una búsqueda continua para desarrollar modelos más eficientes y adaptables. Este proyecto introduce ADAM, dedicado al desarrollo avanzado de arquitecturas y modelos de NLP. ADAM explora enfoques y técnicas novedosas para empujar los límites de lo que los modelos de NLP actuales pueden lograr.

## Trabajo Relacionado
Esta sección revisa la literatura existente sobre LLM, destacando las técnicas comunes y sus limitaciones. También proporciona una visión general de los métodos menos convencionales que han mostrado potencial en estudios recientes, preparando el terreno para las innovaciones introducidas en ADAM.

## Metodología
ADAM emplea un enfoque multifacético, integrando varias técnicas avanzadas:
- **Modelos de Memoria Dinámica**: Exploración de modelos que utilizan memoria dinámica para manejo de dependencias a largo plazo.
- **Transformadores Reversibles**: Implementación de capas invertibles para reducir el uso de memoria durante el entrenamiento.
- **Mecanismos de Atención Jerárquica**: Desarrollo de sistemas de atención multinivel para una mejor comprensión del contexto.
- **Arquitecturas Híbridas**: Combinación de diferentes arquitecturas de modelos para aprovechar sus fortalezas individuales.

## Experimentos y Resultados
- Investigando.

## Discusión
- Via slack.

## Conclusión
- N/A

## Referencias
### Libros
1. "Deep Learning" - Ian Goodfellow, Yoshua Bengio, Aaron Courville
2. "Speech and Language Processing" - Daniel Jurafsky, James H. Martin
3. "Neural Network Methods in Natural Language Processing" - Yoav Goldberg
4. "Natural Language Processing with Python" - Steven Bird, Ewan Klein, Edward Loper
5. "Pattern Recognition and Machine Learning" - Christopher M. Bishop
6. "Artificial Intelligence: A Modern Approach" - Stuart Russell, Peter Norvig
7. "Introduction to Information Retrieval" - Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze
